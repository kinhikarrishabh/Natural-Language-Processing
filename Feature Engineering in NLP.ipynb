{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "000a7459",
   "metadata": {},
   "source": [
    "#    Aim:  Text preprocessing level two operations-Feature engineering of textual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5dc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7199cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_adder(vocab, new_tokens):\n",
    "    \"\"\"\n",
    "    Add new tokens to the vocabulary and sort it.\n",
    "\n",
    "    Args:\n",
    "        vocab (list): The existing vocabulary.\n",
    "        new_tokens (list): New tokens to add to the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated and sorted vocabulary.\n",
    "    \"\"\"\n",
    "    if len(vocab) == 0:\n",
    "        #print(\"IN IF\")\n",
    "        vocab = new_tokens\n",
    "        vocab.sort()\n",
    "    else:\n",
    "        #print(\"IN ELSE\")\n",
    "        temp = [wrd for wrd in new_tokens if wrd not in vocab]\n",
    "        #print(temp)\n",
    "        vocab.extend(temp)\n",
    "        vocab.sort()\n",
    "        #print(vocab)\n",
    "        \n",
    "    return(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df5334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(string, vocab = None):\n",
    "    \"\"\"\n",
    "    Preprocess a string by removing stopwords and optionally updating vocabulary.\n",
    "\n",
    "    Args:\n",
    "        string (str): Input text string to preprocess.\n",
    "        vocab (list, optional): Existing vocabulary to update.\n",
    "\n",
    "    Returns:\n",
    "        list or None: Processed tokens if no vocabulary provided, else updated vocabulary.\n",
    "    \"\"\"\n",
    "    if vocab is not None:\n",
    "        print(string, vocab)\n",
    "        new_tokens = remove_stopword(string)\n",
    "        #print(f\"Length Of Tokens After Stop Word Removal: {len(new_tokens)}\")\n",
    "        #print(type(vocab))\n",
    "        vocab = vocab_adder(vocab, new_tokens)\n",
    "        return(vocab)\n",
    "    else:\n",
    "        new_tokens = remove_stopword(string)\n",
    "        return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e02b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_idf(word, doc):\n",
    "    \"\"\"\n",
    "    Calculate the frequency of a word in a document.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to calculate frequency for.\n",
    "        doc (list): The list of documents.\n",
    "\n",
    "    Returns:\n",
    "        int: Frequency of the word in the document.\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for i in doc:\n",
    "        for wrd in i.split(\" \"):\n",
    "            if wrd == word:\n",
    "                cnt += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ea05a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(string):\n",
    "    \"\"\"\n",
    "    Remove stopwords from a string.\n",
    "\n",
    "    Args:\n",
    "        string (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tokens after removing stopwords.\n",
    "    \"\"\"\n",
    "    string_without_punctuation = remove_punctuation(string)\n",
    "    #print(\"String Without Punctuation: \", string_without_punctuation)\n",
    "    stp = stopwords.words(\"english\")\n",
    "    new_tokens = [wrd for wrd in string_without_punctuation if wrd not in stp]\n",
    "    new_tokens.sort()\n",
    "    #print(new_tokens)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63293e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq(word, corpus):\n",
    "    \"\"\"\n",
    "    Calculate the frequency of a word in a corpus.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to calculate frequency for.\n",
    "        corpus (list): The list of words.\n",
    "\n",
    "    Returns:\n",
    "        int: Frequency of the word in the corpus.\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for wrd in corpus:\n",
    "        if wrd == word:\n",
    "            cnt =+ 1\n",
    "    #print(word, corpus)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563ab3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(string, vocab = None):\n",
    "    \"\"\"\n",
    "    Remove punctuation from a string and update vocabulary if provided.\n",
    "\n",
    "    Args:\n",
    "        string (str): Input text string.\n",
    "        vocab (list, optional): Existing vocabulary to update.\n",
    "\n",
    "    Returns:\n",
    "        list or None: List of tokens after removing punctuation if no vocabulary provided, else updated vocabulary.\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    string_without_punctuation = tokenizer.tokenize(string)\n",
    "    lst = [wrd.lower() for wrd in string_without_punctuation]\n",
    "    lst.sort()\n",
    "    \n",
    "    if vocab is not None:\n",
    "        vocab = vocab_adder(vocab, lst)\n",
    "        return vocab\n",
    "    \n",
    "    else:\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b52a0b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dic(vocab):\n",
    "    \"\"\"\n",
    "    Initialize a dictionary with vocabulary words as keys and initial counts as values.\n",
    "\n",
    "    Args:\n",
    "        vocab (list): Vocabulary list.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with vocabulary words as keys and initial counts as values.\n",
    "    \"\"\"\n",
    "    dic = {}\n",
    "    for i in vocab:\n",
    "        dic[i] = 0\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c2c3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(rev1, vocab):\n",
    "    \"\"\"\n",
    "    Create a Bag of Words representation for a given text using a specified vocabulary.\n",
    "\n",
    "    Args:\n",
    "        rev1 (str): Input text string.\n",
    "        vocab (list): Vocabulary list.\n",
    "\n",
    "    Returns:\n",
    "        dict: Bag of Words representation as a dictionary with words as keys and counts as values.\n",
    "    \"\"\"\n",
    "    cnt = init_dic(vocab)\n",
    "\n",
    "    for wrd in remove_punctuation(rev1):\n",
    "        if wrd in vocab:\n",
    "            x = cnt[wrd]\n",
    "            cnt[wrd] = x+1\n",
    "\n",
    "        else:\n",
    "            cnt[wrd] = 0\n",
    "    \n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ccbc20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input sentences\n",
    "sent1= \"India, country that occupies the greater part of South Asia.\"\n",
    "sent2= \"Its capital is New Delhi.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "067297bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['capital', 'delhi', 'new']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords from a string\n",
    "remove_stopword(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7608c",
   "metadata": {},
   "source": [
    "##  a. To implement label encoding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc991325",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "\n",
    "    -Create a vocabulary from the given corpus. \n",
    "    -Assign a number to each word in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aafa86d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India, country that occupies the greater part of South Asia. []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['asia', 'country', 'greater', 'india', 'occupies', 'part', 'south']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty vocabulary list\n",
    "vocab = []\n",
    "# Preprocess sent1 and update vocabulary\n",
    "vocab = preprocess(sent1, vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d25f55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its capital is New Delhi. ['asia', 'country', 'greater', 'india', 'occupies', 'part', 'south']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['asia',\n",
       " 'capital',\n",
       " 'country',\n",
       " 'delhi',\n",
       " 'greater',\n",
       " 'india',\n",
       " 'new',\n",
       " 'occupies',\n",
       " 'part',\n",
       " 'south']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess sent2 and update vocabulary\n",
    "vocab = preprocess(sent2, vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6352be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asia', 'capital', 'country', 'delhi', 'greater', 'india', 'new', 'occupies', 'part', 'south']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d793430d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asia': 1,\n",
       " 'capital': 2,\n",
       " 'country': 3,\n",
       " 'delhi': 4,\n",
       " 'greater': 5,\n",
       " 'india': 6,\n",
       " 'new': 7,\n",
       " 'occupies': 8,\n",
       " 'part': 9,\n",
       " 'south': 10}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an index dictionary for words in the vocabulary\n",
    "ind = {}\n",
    "for i in range(len(vocab)):\n",
    "    ind[vocab[i]] = i+1\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9b132ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asia', 'country', 'greater', 'india', 'occupies', 'part', 'south']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 6, 8, 9, 10]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and preprocess sent1\n",
    "ls_1 = remove_stopword(sent1)\n",
    "\n",
    "# Perform label encoding for sent1\n",
    "le_1 = []\n",
    "for i in ls_1:\n",
    "    le_1.append(ind[i])\n",
    "\n",
    "# Print the tokenized sentence and label encoded result for sent1\n",
    "print(ls_1)\n",
    "le_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fb88e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['capital', 'delhi', 'new']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 4, 7]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and preprocess sent2\n",
    "ls_2 = remove_stopword(sent2)\n",
    "\n",
    "# Perform label encoding for sent2\n",
    "le_2 = []\n",
    "for i in ls_2:\n",
    "    le_2.append(ind[i])\n",
    "\n",
    "# Print the tokenized sentence and label encoded result for sent2\n",
    "print(ls_2)\n",
    "le_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4e0d2",
   "metadata": {},
   "source": [
    "##  b. To implement one hot encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1ca74",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "    -Create a vocabulary from the given corpus. \n",
    "    -Assign binary vector to each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1a458ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India, country that occupies the greater part of South Asia. []\n",
      "Its capital is New Delhi. ['asia', 'country', 'greater', 'india', 'occupies', 'part', 'south']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['asia',\n",
       " 'capital',\n",
       " 'country',\n",
       " 'delhi',\n",
       " 'greater',\n",
       " 'india',\n",
       " 'new',\n",
       " 'occupies',\n",
       " 'part',\n",
       " 'south']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = []\n",
    "vocab = preprocess(sent1,vocab)\n",
    "vocab = preprocess(sent2,vocab)\n",
    "vocab.sort()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b3a7de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India, country that occupies the greater part of South Asia. []\n",
      "Vocab Of Sentence 1: ['asia', 'country', 'greater', 'india', 'occupies', 'part', 'south']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asia</th>\n",
       "      <th>capital</th>\n",
       "      <th>country</th>\n",
       "      <th>delhi</th>\n",
       "      <th>greater</th>\n",
       "      <th>india</th>\n",
       "      <th>new</th>\n",
       "      <th>occupies</th>\n",
       "      <th>part</th>\n",
       "      <th>south</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asia</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greater</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>india</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occupies</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>part</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          asia  capital  country  delhi  greater  india  new  occupies  part  \\\n",
       "asia         1        0        0      0        0      0    0         0     0   \n",
       "country      0        0        1      0        0      0    0         0     0   \n",
       "greater      0        0        0      0        1      0    0         0     0   \n",
       "india        0        0        0      0        0      1    0         0     0   \n",
       "occupies     0        0        0      0        0      0    0         1     0   \n",
       "part         0        0        0      0        0      0    0         0     1   \n",
       "south        0        0        0      0        0      0    0         0     0   \n",
       "\n",
       "          south  \n",
       "asia          0  \n",
       "country       0  \n",
       "greater       0  \n",
       "india         0  \n",
       "occupies      0  \n",
       "part          0  \n",
       "south         1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a dictionary to store one-hot encoded representation for sent1\n",
    "label_sent1 = {}\n",
    "vocab_sent1=[]\n",
    "vocab_sent1 = preprocess(sent1, vocab_sent1)\n",
    "vocab_sent1.sort()\n",
    "print(f\"Vocab Of Sentence 1: {vocab_sent1}\")\n",
    "\n",
    "# Generate one-hot encoded representation for each word in vocab_sent1\n",
    "for wrd in vocab_sent1:\n",
    "    label_sent1[wrd] = [1 if x==wrd else 0 for x in vocab]\n",
    "\n",
    "# Create a DataFrame to store one-hot encoded representation for sent1\n",
    "df_sent1 = pd.DataFrame(columns = vocab, index=label_sent1.keys(), data=label_sent1.values()) \n",
    "df_sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e313a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its capital is New Delhi. []\n",
      "Vocab Of Sentence 2: ['capital', 'delhi', 'new']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asia</th>\n",
       "      <th>capital</th>\n",
       "      <th>country</th>\n",
       "      <th>delhi</th>\n",
       "      <th>greater</th>\n",
       "      <th>india</th>\n",
       "      <th>new</th>\n",
       "      <th>occupies</th>\n",
       "      <th>part</th>\n",
       "      <th>south</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>capital</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delhi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asia  capital  country  delhi  greater  india  new  occupies  part  \\\n",
       "capital     0        1        0      0        0      0    0         0     0   \n",
       "delhi       0        0        0      1        0      0    0         0     0   \n",
       "new         0        0        0      0        0      0    1         0     0   \n",
       "\n",
       "         south  \n",
       "capital      0  \n",
       "delhi        0  \n",
       "new          0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_sent2 = {}\n",
    "vocab_sent2=[]\n",
    "vocab_sent2 = preprocess(sent2, vocab_sent2)\n",
    "vocab_sent2.sort()\n",
    "print(f\"Vocab Of Sentence 2: {vocab_sent2}\")\n",
    "\n",
    "for wrd in vocab_sent2:\n",
    "    label_sent2[wrd] = [1 if x==wrd else 0 for x in vocab]\n",
    "\n",
    "df_sent2 = pd.DataFrame(columns = vocab, index=label_sent2.keys(), data=label_sent2.values()) \n",
    "df_sent2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe8be4",
   "metadata": {},
   "source": [
    "## c. To implement BoW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca48c54",
   "metadata": {},
   "source": [
    "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "    1) A vocabulary of known words.\n",
    "    2) A measure of the presence of known words.\n",
    "\n",
    "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document\n",
    "\n",
    "\n",
    "A very common feature extraction procedures for sentences and documents is the bag-of-words approach (BOW). In this approach, we look at the histogram of the words within the text, i.e. considering each word count as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7962073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev1 = \"Game of Thrones is an amazing tv series!\"\n",
    "rev2 = \"Game of Thrones is the best tv series!\"\n",
    "rev3 = \"Game of Thrones is so great\"\n",
    "rev4 = \"game game of of thrones is is great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e762515e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Game', 'of', 'Thrones', 'is', 'an', 'amazing', 'tv', 'series!']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev1.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc8f3f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'an',\n",
       " 'best',\n",
       " 'game',\n",
       " 'great',\n",
       " 'is',\n",
       " 'of',\n",
       " 'series',\n",
       " 'so',\n",
       " 'the',\n",
       " 'thrones',\n",
       " 'tv']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vocab = []\n",
    "bow_vocab = remove_punctuation(rev1, bow_vocab)\n",
    "bow_vocab = remove_punctuation(rev2, bow_vocab)\n",
    "bow_vocab = remove_punctuation(rev3, bow_vocab)\n",
    "bow_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "536960fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game of Thrones is an amazing tv series!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'amazing': 1,\n",
       " 'an': 1,\n",
       " 'best': 0,\n",
       " 'game': 1,\n",
       " 'great': 0,\n",
       " 'is': 1,\n",
       " 'of': 1,\n",
       " 'series': 1,\n",
       " 'so': 0,\n",
       " 'the': 0,\n",
       " 'thrones': 1,\n",
       " 'tv': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt1 = bow(rev1, bow_vocab)\n",
    "print(rev1)\n",
    "cnt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ca5c5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game of Thrones is the best tv series!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'amazing': 0,\n",
       " 'an': 0,\n",
       " 'best': 1,\n",
       " 'game': 1,\n",
       " 'great': 0,\n",
       " 'is': 1,\n",
       " 'of': 1,\n",
       " 'series': 1,\n",
       " 'so': 0,\n",
       " 'the': 1,\n",
       " 'thrones': 1,\n",
       " 'tv': 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt2 = bow(rev2, bow_vocab)\n",
    "print(rev2)\n",
    "cnt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa3e1399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game of Thrones is so great\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'amazing': 0,\n",
       " 'an': 0,\n",
       " 'best': 0,\n",
       " 'game': 1,\n",
       " 'great': 1,\n",
       " 'is': 1,\n",
       " 'of': 1,\n",
       " 'series': 0,\n",
       " 'so': 1,\n",
       " 'the': 0,\n",
       " 'thrones': 1,\n",
       " 'tv': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt3 = bow(rev3, bow_vocab)\n",
    "print(rev3)\n",
    "cnt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc0be4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game game of of thrones is is great\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'amazing': 0,\n",
       " 'an': 0,\n",
       " 'best': 0,\n",
       " 'game': 2,\n",
       " 'great': 1,\n",
       " 'is': 2,\n",
       " 'of': 2,\n",
       " 'series': 0,\n",
       " 'so': 0,\n",
       " 'the': 0,\n",
       " 'thrones': 1,\n",
       " 'tv': 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt4 = bow(rev4, bow_vocab)\n",
    "print(rev4)\n",
    "cnt4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270f3bb",
   "metadata": {},
   "source": [
    "## d. To implement TF-IDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e5587",
   "metadata": {},
   "source": [
    "TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set)\n",
    "Idfgivesrelevace of  word in  the  corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bdd3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "doc1 = \"Inflation has increased unemployement.\"\n",
    "doc2 = \"The company has increased its sales\"\n",
    "doc3 = \"Fear increased his pulse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48cd0f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['increased inflation unemployement',\n",
       " 'company increased sales',\n",
       " 'fear increased pulse']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess each document and create cleaned versions\n",
    "doc1_clean = preprocess(doc1)\n",
    "doc2_clean = preprocess(doc2)\n",
    "doc3_clean = preprocess(doc3)\n",
    "\n",
    "# Combine cleaned documents into a list\n",
    "doc = [' '.join(doc1_clean), ' '.join(doc2_clean), ' '.join(doc3_clean)]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79861f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflation has increased unemployement. []\n",
      "The company has increased its sales ['increased', 'inflation', 'unemployement']\n",
      "Fear increased his pulse ['company', 'increased', 'inflation', 'sales', 'unemployement']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['company',\n",
       " 'fear',\n",
       " 'increased',\n",
       " 'inflation',\n",
       " 'pulse',\n",
       " 'sales',\n",
       " 'unemployement']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty vocabulary list for TF calculations\n",
    "tf_vocab = []\n",
    "\n",
    "# Preprocess each document and update vocabulary for TF calculations\n",
    "tf_vocab = preprocess(doc1, tf_vocab)\n",
    "tf_vocab = preprocess(doc2, tf_vocab)\n",
    "tf_vocab = preprocess(doc3, tf_vocab)\n",
    "tf_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a248d6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': array([0.        , 0.33333333, 0.        ]),\n",
       " 'fear': array([0.        , 0.        , 0.33333333]),\n",
       " 'increased': array([0.33333333, 0.33333333, 0.33333333]),\n",
       " 'inflation': array([0.33333333, 0.        , 0.        ]),\n",
       " 'pulse': array([0.        , 0.        , 0.33333333]),\n",
       " 'sales': array([0.        , 0.33333333, 0.        ]),\n",
       " 'unemployement': array([0.33333333, 0.        , 0.        ])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TF scores for each word in the vocabulary\n",
    "tf_score = {}\n",
    "for i in tf_vocab:\n",
    "    tf_score[i] = np.array([freq(i, doc1_clean), freq(i, doc2_clean), freq(i, doc3_clean)])/len(doc)\n",
    "tf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07570a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>increased inflation unemployement</th>\n",
       "      <th>company increased sales</th>\n",
       "      <th>fear increased pulse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>increased</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inflation</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulse</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unemployement</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               increased inflation unemployement  company increased sales  \\\n",
       "company                                 0.000000                 0.333333   \n",
       "fear                                    0.000000                 0.000000   \n",
       "increased                               0.333333                 0.333333   \n",
       "inflation                               0.333333                 0.000000   \n",
       "pulse                                   0.000000                 0.000000   \n",
       "sales                                   0.000000                 0.333333   \n",
       "unemployement                           0.333333                 0.000000   \n",
       "\n",
       "               fear increased pulse  \n",
       "company                    0.000000  \n",
       "fear                       0.333333  \n",
       "increased                  0.333333  \n",
       "inflation                  0.000000  \n",
       "pulse                      0.333333  \n",
       "sales                      0.000000  \n",
       "unemployement              0.000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to display TF scores for each word in each document\n",
    "pd.DataFrame(columns=doc, index=tf_score.keys(), data = tf_score.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10bb3dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': 0.47712125471966244,\n",
       " 'fear': 0.47712125471966244,\n",
       " 'increased': 0.0,\n",
       " 'inflation': 0.47712125471966244,\n",
       " 'pulse': 0.47712125471966244,\n",
       " 'sales': 0.47712125471966244,\n",
       " 'unemployement': 0.47712125471966244}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate IDF scores for each word in the vocabulary\n",
    "idf_score = {}\n",
    "for i in tf_vocab:\n",
    "    idf_score[i] = np.log10(len(doc)/freq_idf(i, doc))  \n",
    "idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9eeeb614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>0.477121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.477121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>increased</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inflation</th>\n",
       "      <td>0.477121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulse</th>\n",
       "      <td>0.477121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales</th>\n",
       "      <td>0.477121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unemployement</th>\n",
       "      <td>0.477121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               IDF Score\n",
       "company         0.477121\n",
       "fear            0.477121\n",
       "increased       0.000000\n",
       "inflation       0.477121\n",
       "pulse           0.477121\n",
       "sales           0.477121\n",
       "unemployement   0.477121"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(columns=[\"IDF Score\"], index=idf_score.keys(), data=idf_score.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94097269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': array([0.        , 0.15904042, 0.        ]),\n",
       " 'fear': array([0.        , 0.        , 0.15904042]),\n",
       " 'increased': array([0., 0., 0.]),\n",
       " 'inflation': array([0.15904042, 0.        , 0.        ]),\n",
       " 'pulse': array([0.        , 0.        , 0.15904042]),\n",
       " 'sales': array([0.        , 0.15904042, 0.        ]),\n",
       " 'unemployement': array([0.15904042, 0.        , 0.        ])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = {}\n",
    "for i in idf_score:\n",
    "    tf_idf[i] = tf_score[i]*idf_score[i]\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45320b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>increased</th>\n",
       "      <th>inflation</th>\n",
       "      <th>unemployement</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>increased</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>fear</th>\n",
       "      <th>increased</th>\n",
       "      <th>pulse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15904</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>increased</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inflation</th>\n",
       "      <td>0.15904</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulse</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.15904</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unemployement</th>\n",
       "      <td>0.15904</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              increased inflation unemployement\n",
       "                company increased         sales\n",
       "                   fear increased         pulse\n",
       "company         0.00000   0.15904       0.00000\n",
       "fear            0.00000   0.00000       0.15904\n",
       "increased       0.00000   0.00000       0.00000\n",
       "inflation       0.15904   0.00000       0.00000\n",
       "pulse           0.00000   0.00000       0.15904\n",
       "sales           0.00000   0.15904       0.00000\n",
       "unemployement   0.15904   0.00000       0.00000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(index=tf_idf.keys(), columns=[doc1_clean, doc2_clean, doc3_clean], data=tf_idf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09593359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_idf(\"increased\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee47f12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['increased inflation unemployement',\n",
       " 'company increased sales',\n",
       " 'fear increased pulse']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d6b4521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'company'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d23337f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq(preprocess(doc1)[0], tf_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a50f27",
   "metadata": {},
   "source": [
    "##  e. Explore Scikit learn to implement TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf6ac051",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"\"\"Inflation has increased unemployement. The company has increased its sales\n",
    "Fear increased his pulse\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4282c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.21320071635561041\n",
      "  (0, 3)\t0.21320071635561041\n",
      "  (0, 1)\t0.21320071635561041\n",
      "  (0, 8)\t0.21320071635561041\n",
      "  (0, 6)\t0.21320071635561041\n",
      "  (0, 0)\t0.21320071635561041\n",
      "  (0, 9)\t0.21320071635561041\n",
      "  (0, 10)\t0.21320071635561041\n",
      "  (0, 4)\t0.6396021490668313\n",
      "  (0, 2)\t0.42640143271122083\n",
      "  (0, 5)\t0.21320071635561041\n"
     ]
    }
   ],
   "source": [
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "sent=[sentence]\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(sent)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e2688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdff0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b9e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e316538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
